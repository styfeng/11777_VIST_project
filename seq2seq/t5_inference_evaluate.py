# -*- coding: utf-8 -*-
"""T5_Inference_Evaluate.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GRGSc-SAMO8s9dwQiBw5L82RXJ3NSqgS
"""

import os
import sys

#!pip install --no-cache-dir transformers==4.3.3 sentencepiece==0.1.95
#!pip install spacy==2.1.0
#!pip install torch
#!pip install datasets
#!pip install rouge_score
#!pip install pip install sentencepiece
#!pip install gitpython
#!pip install sacrebleu
#!pip install sentence_splitter==1.4
#!pip install neuralcoref==4.0.0
#!python -m spacy download en
#!python -m spacy validate
#!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz

"""# CHANGE THIS CELL BELOW"""

#CHANGE values in this cell appropriately
size = sys.argv[1] #CHANGE: IMPORTANT: either 'base' or 'large', base model is smaller and quicker to train and may be sufficient for your purposes. especially since T5-large is much much slower and compute heavy thane T5-base due to having to disable fp16
lr = float(sys.argv[2]) #CHANGE: IMPORTANT: need to choose a good learning rate. some reasonable values to try are 5e-06, 1e-05, 2e-05, 3e-05, 5e-05
default_seed=int(sys.argv[3]) #CHANGE: IMPORTANT: seed1: 42, seed2: 24 (typically you train two seeds/versions of a model and take average, for your purposes probably enough to stick with one seed)
ENCODER_MAX_LENGTH = int(sys.argv[4]) #CHANGE: IMPORTANT: set this to reasonable value to ensure most input texts in your dataset can fit onto the encoder model. this is the number of tokens, but you can approximate it with the number of words in the input texts. reasonable values: 32, 64, 128 (powers of 2). 64 should be enough for most purposes...
DECODER_MAX_LENGTH = int(sys.argv[5]) #CHANGE: IMPORTANT: set this to reasonable value to ensure most output texts in your dataset can fit onto the decoder model. this is the number of tokens, but you can approximate it with the number of words in the output texts. reasonable values: 32, 64, 128 (powers of 2). 64 should be enough for most purposes...
model_type = sys.argv[6] #CHANGE: a string to describe the type of trained model, e.g. 'baseline' vs. 'top1'
val_json_filename = sys.argv[7]
GPU = sys.argv[8]
os.environ["CUDA_VISIBLE_DEVICES"] = GPU
checkpoint = sys.argv[9]
base_folder = f"trained_T5_models/T5-{size}_{lr}_{default_seed}_{model_type}" #IMPORTANT: model folder that contains the checkpoint folders
input_batch_size = int(sys.argv[10]) #DEFAULT = 32


"""# Preliminary Things to Run"""

input_model = "{}/checkpoint-{}".format(base_folder,checkpoint) #full path to input model
model_string = f"T5-{size}_{lr}_{default_seed}_{model_type}" #string describing model and its parameters to be used in the variables below

#Note that likely the two most important evaluation metrics would be ROUGE and BERTScore (for both metrics, higher = better)
    #For BERTScore three values are outputted (the third value is called F1 and is the important one)
    #For ROUGE, three values (ROUGE1, ROUGE2, ROUGEL) are also outputted, and all three are important. However, ROUGE2 is typically seen as most important.
out_generations_filename_txt = f"{base_folder}/test_generated_{model_string}_cp{checkpoint}_txt.txt" #IMPORTANT: .txt file to save generated summaries to
out_generations_filename = f"{base_folder}/test_generated_{model_string}_cp{checkpoint}_json.json" #IMPORTANT: .json file to save generated summaries to
out_ROUGE_name = f"{base_folder}/test_generated_{model_string}_cp{checkpoint}_ROUGE.json" #IMPORTANT: file to save individual ROUGE results to
out_BERTScore_name = f"{base_folder}/test_generated_{model_string}_cp{checkpoint}_BERTScores.json" #IMPORTANT: file to save individual BERTScore results to
#out_METEOR_name = f"{base_folder}/test_generated_{model_string}_cp{checkpoint}_METEOR.json" #IMPORTANT: file to save individual METEOR results to
out_diversity_name = f"{base_folder}/test_generated_{model_string}_cp{checkpoint}_diversity.json" #IMPORTANT: file to save individual length + diversity results to
out_stats_name = f"{base_folder}/test_generated_{model_string}_cp{checkpoint}_stats.txt" #IMPORTANT: file to save overall average results to

import torch
from packaging import version
if version.parse(torch.__version__) < version.parse("1.6"):
    from .file_utils import is_apex_available
    if is_apex_available():
        from apex import amp
    _use_apex = True
else:
    _use_native_amp = True
    from torch.cuda.amp import autocast


from datasets import load_dataset, load_metric                                               
#from torch.cuda.amp import autocast # need torch >=1.6     
try:                     
    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
except:
    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM                  
from transformers import Trainer, TrainingArguments
from transformers import BartForConditionalGeneration, BartTokenizer, T5ForConditionalGeneration

import transformers
print(transformers.__version__)

t5_tokenizer = AutoTokenizer.from_pretrained(f"t5-{size}", use_fast=False)

"""# Load CommonGen Data into HuggingFace Dataset"""

import json

# load data from json file. format: list of dictionaries, each containing one example, where keys of each dict are "document", "edited_key" (specified as argument), "id", and "summary" (and possibly additional keys, these are just the mandatory ones)
def load_from_json(filename):
    with open(filename) as f:
        loaded_json = json.loads(f.read())
    #print(type(loaded_json))
    #print(loaded_json[0])
    inputs = [x['input'] for x in loaded_json]
    #print('input:', inputs[0])
    outputs = [x['output'] for x in loaded_json]
    #print('output:', outputs[0])
    return loaded_json

loaded_val_json = load_from_json(val_json_filename)

#modified : this cell has been inserted

#convert loaded json format to format required to load into huggingface dataset. edited_key argument same as earlier
def convert_json_format(loaded_json):#(loaded_train_json,loaded_val_json):
    loaded_dict = {}
    input_lst = [x['input'] for x in loaded_json]
    #print(len(input_lst))
    loaded_dict['input'] = input_lst
    #print(loaded_dict['input'][:3])
    output_lst = [x['output'] for x in loaded_json]
    #print(len(output_lst))
    loaded_dict['output'] = output_lst
    #print(loaded_dict['output'][:3])
    return loaded_dict

converted_val_json = convert_json_format(loaded_val_json)

#modified : this cell has been inserted

from datasets import Dataset

#convert the converted_json to huggingface dataset format
def load_huggingface_dataset_from_dict(converted_json):
    #train_dataset = Dataset.from_dict(converted_json["train"])
    #val_dataset = Dataset.from_dict(converted_json["validation"])
    loaded_hf_dataset = Dataset.from_dict(converted_json)
    #print(type(loaded_hf_dataset))
    #print(loaded_hf_dataset)
    return loaded_hf_dataset

loaded_hf_dataset = load_huggingface_dataset_from_dict(converted_val_json)

print(loaded_hf_dataset[:2])

"""#Generation / Decoding"""

LANGUAGE = 'en' #maybe variable not required (e.g. not in config) for non-FB models
#LANGUAGE = 'es_XX'
# Fairseq language codes for the mlsum languages are: es_XX, fr_XX, de_DE, ru_RU, tr_TR

BEAM_SIZE = 5 #CHANGED : was 2
DECODER_EARLY_STOPPING = True #CHANGED : added #Vestigial for Pegasus
DECODER_LENGTH_PENALTY = 0.6 #CHANGED : added
DECODER_MIN_LENGTH = 1 #CHANGED : added
#NO_REPEAT_NGRAM_SIZE = None
#PREFIX = None
NO_REPEAT_NGRAM_SIZE = 3
#PREFIX = "summarize: "
device = 'cuda'

import nltk; nltk.download('punkt')
from transformers import T5Config

#load the model from the specified folder:
t5_model = T5ForConditionalGeneration.from_pretrained(input_model)

#below lines are for attempting to load from tensorflow checkpoint (doesn't work so far):
#config = T5Config.from_pretrained(input_model)
#t5_model = T5ForConditionalGeneration.from_pretrained(input_model,config=config,from_tf=True)

#NOTE: this fixes a bug in v. 4.1.1 where the mBART model checkpoints are not saved fully. It does not affect 4.2.
if version.parse(transformers.__version__) <= version.parse("4.1.1"):
    t5_model._keys_to_ignore_on_load_missing = None
    t5_model._keys_to_ignore_on_save = None
t5_model.to(device)

# Utility functions for generating text from the model.
#below 3 functions to create decoding time minibatches

def make_batch(texts, tokenizer,  device, src_lang=LANGUAGE):
    """ texts is the list of strings to use as input.
    LANGUAGE is the fairseq language code used (e.g., "es_XX", "fr_XX", "de_DE",
      "ru_RU", "tr_TR").
    """
    batch = tokenizer.prepare_seq2seq_batch(src_texts = texts, src_lang=src_lang, max_length=ENCODER_MAX_LENGTH, padding='max_length', return_tensors='pt',truncation=True)
    batch_features = dict([(k, v.to(device)) for k, v in batch.items()])
    return batch_features

#NOTE 250005 is the lang_id for Spanish.
#CHANGED : note that two functions below are changed to include additional arguments (e.g. min_length)
def generate(batch_features,
             model,
             tokenizer,
             early_stopping=DECODER_EARLY_STOPPING,
             length_penalty=DECODER_LENGTH_PENALTY,
             min_length=DECODER_MIN_LENGTH,
             max_length=DECODER_MAX_LENGTH,
             no_repeat_ngram_size=NO_REPEAT_NGRAM_SIZE, #added
             num_beams=BEAM_SIZE,
             src_lang=LANGUAGE):
    lang_id = tokenizer.encode(src_lang)[0]
    outputs = model.generate(**batch_features,
                decoder_start_token_id=0, #for T5
                num_beams=num_beams,
                length_penalty = length_penalty,
                early_stopping = early_stopping,
                min_length = min_length,
                max_length=max_length,
                no_repeat_ngram_size = no_repeat_ngram_size #added
                )
    out = tokenizer.batch_decode(outputs, skip_special_tokens=True)
    return out

#this function is especially important, need to put the right field (e.g. input_key like "document" needs to be correct, 'document' is the default)
#modified: input_key is now an argument rather than hardcoded, will be set later when calling "generate_from_hf_batch" function
def generate_from_hf_data(batch,
                          tokenizer,
                          model,
                          device,
                          src_lang=LANGUAGE,
                          early_stopping=DECODER_EARLY_STOPPING,
                          length_penalty=DECODER_LENGTH_PENALTY,                         
                          min_length=DECODER_MIN_LENGTH,
                          max_length=DECODER_MAX_LENGTH,
                          no_repeat_ngram_size=NO_REPEAT_NGRAM_SIZE, #added
                          num_beams=BEAM_SIZE,
                          input_key='input'):
    #texts = batch['document']
    #texts = batch['edited_document']
    texts = batch[input_key]
    batch_features = make_batch(texts, tokenizer, device, src_lang=LANGUAGE)
    out = generate(batch_features,
                   model,
                   tokenizer,
                   early_stopping=early_stopping,
                   length_penalty=length_penalty,
                   min_length=min_length,
                   max_length=max_length,
                   no_repeat_ngram_size=no_repeat_ngram_size, #added
                   num_beams=num_beams,
                   src_lang=src_lang)
    return out

#modified : this is a copy of the above commented out cell (from Varun's original notebook) but instead of map function over xsum['validation'], it maps over "loaded_hf_dataset" (the loaded huggingface dataset)
#also, input_key is now an argument to generate_from_hf_data, and change in_key variable as necessary
#time required: around 20 mins using V100 (or 35 mins using P100, 55 mins T4) for entire validation split using bartpretrained (11332 generations)

print(BEAM_SIZE)
valid_output_beam4 = None
in_key = 'input'

#takes around 2 minutes for full validation split using P100
valid_output_beam5_64 = loaded_hf_dataset.map(
        lambda batch: {'generated': generate_from_hf_data(batch,
            t5_tokenizer,
            t5_model,
            device,
            src_lang=LANGUAGE,
            early_stopping=DECODER_EARLY_STOPPING,
            length_penalty=DECODER_LENGTH_PENALTY,
            min_length=DECODER_MIN_LENGTH,
            max_length=DECODER_MAX_LENGTH,
            no_repeat_ngram_size=NO_REPEAT_NGRAM_SIZE,
            num_beams=BEAM_SIZE,
            input_key=in_key)
            },
            batched=True,
            batch_size=input_batch_size)

#print(valid_output_beam5_64)
print(valid_output_beam5_64["input"][:2])
print(valid_output_beam5_64["output"][:2]) #comment if test set
print(valid_output_beam5_64["generated"][:2])
print(len(valid_output_beam5_64["generated"]))

#Save Output Generations to .json File
import json
with open(out_generations_filename,"w") as f:
   json.dump(valid_output_beam5_64[:],f,indent=4)
f.close()

#Save output generations to .txt file
with open(out_generations_filename_txt,'w') as f:
    f.write('\n'.join(valid_output_beam5_64["generated"][:len(valid_output_beam5_64["generated"])]))
f.close()

print("generated text written to files!")

"""#Evaluation"""

#modified : cell inserted. allows to either calculate metrics based on outputs from current run of notebook or to calculate from generations in a .json file

#below binary variable set to True if reading generations from file, otherwise False if just calculating based on outputs from current run of the notebook
calculate_from_file = False #CHANGE accordingly, probably leave as False
import json

# load data from json file with generations. format: dictionary with keys ['document', 'edited_document', 'generated', 'id', 'summary'], and values as corresponding lists
def load_generations_from_json(filename):
    with open(filename) as f:
        loaded_json = json.loads(f.read())
    print(type(loaded_json))
    generations = loaded_json['generated']
    gt_outputs = loaded_json['output']
    print('generations:', generations[:10])
    print('gt_outputs:', gt_outputs[:10])
    print('generations:', generations[-10:])
    print('gt_outputs:', gt_outputs[-10:])
    print(len(generations))
    print(len(gt_outputs))
    return generations, gt_outputs

def load_generations_from_txt(gt_filename,gen_filename):
    f1 = open(gt_filename,'r')
    gt_outputs = [x.strip() for x in f1.readlines()]
    f2 = open(gen_filename,'r')
    generations = [x.strip() for x in f2.readlines()]
    print('generations:', generations[:10])
    print('gt_outputs:', gt_outputs[:10])
    print('generations:', generations[-10:])
    print('gt_outputs:', gt_outputs[-10:])
    print(len(generations))
    print(len(gt_outputs))
    return generations, gt_outputs

if calculate_from_file is True:
    dev_tgt_filename = "commongen_data/commongen.dev.tgt.txt"
    generations_filename = "commongen_data/keyphrases/BART-large_mask_infill_val_generated_steven_colab_cp5797_kw_yake_ngram5_permuted_full_single.txt" #CHANGE appropriately
    #generations, gt_outputs = load_generations_from_json(generations_filename)
    generations, gt_outputs = load_generations_from_txt(dev_tgt_filename,generations_filename)

"""# ROUGE Evaluation"""

from datasets import load_dataset, load_metric  
rouge_scorer = load_metric("rouge")

#modified : inserted cell to store separate ROUGE results per example

if calculate_from_file is False:

    rouge_results_beam5_64 = rouge_scorer.compute(
        predictions=valid_output_beam5_64["generated"],
        references=valid_output_beam5_64["output"],
        rouge_types=["rouge1", "rouge2", "rougeL"],
        use_agregator=False, use_stemmer=False,
    )

else:
    rouge_results_beam5_64 = rouge_scorer.compute(
        predictions=generations,
        references=gt_outputs,
        rouge_types=["rouge1", "rouge2", "rougeL"],
        use_agregator=False, use_stemmer=False,
    )

# Individual ROUGE-1/2/L
print(len(rouge_results_beam5_64['rouge1']))
print(len(rouge_results_beam5_64['rouge2']))
print(len(rouge_results_beam5_64['rougeL']))
#print(rouge_results_beam5_64['rouge1'])
#print(rouge_results_beam5_64['rouge2'])
#print(rouge_results_beam5_64['rougeL'])

# modified : cell inserted to save individual ROUGE results to a file

ROUGE_dict = {}
ROUGE_dict["rouge1"] = [x.fmeasure for x in rouge_results_beam5_64['rouge1']]
ROUGE_dict["rouge2"] = [x.fmeasure for x in rouge_results_beam5_64['rouge2']]
ROUGE_dict["rougeL"] = [x.fmeasure for x in rouge_results_beam5_64['rougeL']]

#save individual ROUGE scores to file below. please uncomment the last three lines of this cell to save
#out_ROUGE_name = "split/metrics/valid_outputs_bartpretrained_0.8_split_full_ROUGE.json"

import json
with open(out_ROUGE_name,"w") as ROUGE_f:
    json.dump(ROUGE_dict,ROUGE_f,indent=4)
ROUGE_f.close()
print("ROUGE scores written to file")

#modified : calculate ROUGE based on outputs from current run of notebook OR from generations in a .json file (based on variable in previous cell)

if calculate_from_file is False:

    rouge_results_beam5_64 = rouge_scorer.compute(
        predictions=valid_output_beam5_64["generated"],
        references=valid_output_beam5_64["output"],
        rouge_types=["rouge1", "rouge2", "rougeL"],
        use_agregator=True, use_stemmer=False,
    )

else:
    rouge_results_beam5_64 = rouge_scorer.compute(
        predictions=generations,
        references=gt_outputs,
        rouge_types=["rouge1", "rouge2", "rougeL"],
        use_agregator=True, use_stemmer=False,
    )

# ROUGE-2/L
print(rouge_results_beam5_64['rouge1'])
print(rouge_results_beam5_64['rouge2'])
print(rouge_results_beam5_64['rougeL'])
print("Final average results: ", rouge_results_beam5_64['rouge1'].mid.fmeasure, rouge_results_beam5_64['rouge2'].mid.fmeasure, rouge_results_beam5_64['rougeL'].mid.fmeasure)


"""# BERTScore Evaluation"""

#modified : below cells are all inserted for purpose of BERTScore evaluation

#!pip install bert_score==0.3.8 #changed to specific version

import bert_score
from bert_score import BERTScorer

def create_scorer():
    # Create scorer object for passing to get_bert_score
    scorer = BERTScorer(lang="en", rescale_with_baseline=True, model_type='roberta-base')
    return scorer

def get_bert_score(hyp,ref,scorer):
    # hyp: hypothesis ref: reference scorer: Already created BERT Score object
    # Returns F1: BERT-Score F1 between hypothesis and reference
    # Note: Some settings need to be done while creating the scorer object e.g whether to normalize by baseline or not, or which BERT model to use
    hyp = hyp.strip()
    ref = ref.strip()
    P, R, F1 = scorer.score([hyp,],[ref,])
    P = float(P.data.cpu().numpy())
    R = float(R.data.cpu().numpy())
    F1 = float(F1.data.cpu().numpy())
    return P, R, F1

from tqdm import tqdm
import numpy as np

def evaluate_bertscore(references, generations, scorer):
    all_results_P = []
    all_results_R = []
    all_results_F1 = []
    for ref, gen in tqdm(zip(references,generations)):
        P, R, F1 = get_bert_score(gen,ref,scorer)
        all_results_P.append(P)
        all_results_R.append(R)
        all_results_F1.append(F1)
    final_P = np.average(all_results_P)
    final_R = np.average(all_results_R)
    final_F1 = np.average(all_results_F1)
    return all_results_P, all_results_R, all_results_F1, final_P, final_R, final_F1

scorer = create_scorer()

# evaluate BERTScore on the validation set. either based on outputs from current run of notebook OR from generations in a .json file (based on variable in earlier cell)
# note: BERTScore values may differ *very slightly* depending on run of the notebook for the same data

if calculate_from_file is False:
    all_results_P, all_results_R, all_results_F1, final_P, final_R, final_F1 = evaluate_bertscore(valid_output_beam5_64["output"], valid_output_beam5_64["generated"], scorer)
else:
    all_results_P, all_results_R, all_results_F1, final_P, final_R, final_F1 = evaluate_bertscore(gt_outputs, generations, scorer)

print(final_P, final_R, final_F1) #around 2 minutes to run on full 11332 validation examples. final_F1 is the important score to look at

# save individual BERTScore results to a file

BERTScores_dict = {}
BERTScores_dict["P"] = all_results_P
BERTScores_dict["R"] = all_results_R
BERTScores_dict["F1"] = all_results_F1

#save individual BERTScores to file below. please uncomment the last three lines of this cell to save

import json
with open(out_BERTScore_name,"w") as BERTScore_f:
    json.dump(BERTScores_dict,BERTScore_f,indent=4)
BERTScore_f.close()
print("BERTScore results written to file")

'''
"""# METEOR Score"""

!pip install --upgrade nltk

!pip install meteor
meteor_scorer = load_metric('meteor')

import nltk
from nltk import word_tokenize
nltk.download('punkt')
nltk.download('wordnet')

from nltk.translate import meteor_score
#note: METEOR is not symmetric (unlike all the other metrics)

def calc_meteor_score(pred, ref, alpha=0.9, beta=3, gamma=0.5):
    assert len(pred) == len(ref)
    meteor_scores = [meteor_score.single_meteor_score(r, p, alpha=alpha, beta=beta, gamma=gamma) for r, p in tqdm(zip(ref, pred),total=len(ref))]
    return meteor_scores

if calculate_from_file is False:
    all_meteor_scores = calc_meteor_score(valid_output_beam5_64["generated"], valid_output_beam5_64["output"])
else:
    all_meteor_scores = calc_meteor_score(generations, gt_outputs)

print(np.average(all_meteor_scores))

# save individual METEOR results to a file

METEOR_dict = {}
METEOR_dict["METEOR"] = all_meteor_scores

import json
with open(os.path.join(main_path, out_METEOR_name),"w") as METEOR_f:
    json.dump(METEOR_dict,METEOR_f,indent=4)
METEOR_f.close()
print("METEOR results written to file")
'''

"""# Diversity Metrics"""

#functions for length, TTR, UTR

from string import punctuation
import re
import pkg_resources
import numpy as np
import nltk
nltk.download('punkt')
from nltk import word_tokenize


def get_length(document):
    word_lst = word_tokenize(document)
    return len(word_lst)


#get type-token ratio of document
def TTR_score(document):
    word_lst = word_tokenize(document)
    clean_word_lst = []

    for word in word_lst:
        clean_word_lst.append(word)

    unique_word_lst = set(clean_word_lst)
    if len(clean_word_lst) == 0:
        TTR = 0
    else:
        TTR = len(unique_word_lst) / len(clean_word_lst)
    #print("Document: ", document, " / TTR: ", TTR)
    return TTR


#get unique-trigram ratio of document
def UTR_score(document):
    # returns the unique trigram fraction in this population.
    # Higher the unique trigram fraction, more the diversity
    unique_trigrams = set()
    total_trigrams = 0

    #for i,hyp_i in enumerate(hyp_population):
    document_words = document.strip().split()
    if len(document_words)>=3:
        total_trigrams += len(document_words)-2
        for j in range(len(document_words)-2):
            trigram = " ".join(document_words[j:j+2])
            unique_trigrams.add(trigram)

    unique_trigram_fraction = len(unique_trigrams)/(total_trigrams+1e-10)
    if total_trigrams == 0: unique_trigram_fraction = 0.0
    return unique_trigram_fraction

import time
from tqdm.auto import tqdm, trange

#get length, TTR, and UTR of documents
if calculate_from_file is False:
    document_val_char_len = [len(doc) for doc in valid_output_beam5_64["generated"]]
    document_val_len = [get_length(doc) for doc in valid_output_beam5_64["generated"]]
    document_val_TTR = [TTR_score(doc) for doc in tqdm(valid_output_beam5_64["generated"])]
    document_val_UTR = [UTR_score(doc) for doc in tqdm(valid_output_beam5_64["generated"])]
else:
    document_val_char_len = [len(doc) for doc in generations]
    document_val_len = [get_length(doc) for doc in generations]
    document_val_TTR = [TTR_score(doc) for doc in generations]
    document_val_UTR = [UTR_score(doc) for doc in generations]    

print("Average results: {} char_len, {} word_len, {} TTR, {} UTR".format(np.average(document_val_char_len),np.average(document_val_len),np.average(document_val_TTR),np.average(document_val_UTR)))
print(document_val_len[0:20])
print(min(document_val_len)) #3
print(max(document_val_len)) #16
#print(len(document_val_len))
print(document_val_TTR[0:20])
print(min(document_val_TTR)) #0.7272727272727273
#print(len(document_val_TTR))
print(document_val_UTR[0:20])
print(min(document_val_UTR)) #0.8888888888790123
#print(len(document_val_UTR))

# save individual diversity results to a file

diversity_dict = {}
diversity_dict["char_length"] = document_val_char_len
diversity_dict["word_length"] = document_val_len
diversity_dict["TTR"] = document_val_TTR
diversity_dict["UTR"] = document_val_UTR

import json
with open(out_diversity_name,"w") as diversity_f:
    json.dump(diversity_dict,diversity_f,indent=4)
diversity_f.close()
print("Diversity results written to file")


#save overall average results/stats to a file

with open(out_stats_name,"w") as stats_f:
    stats_f.write(str(rouge_results_beam5_64['rouge1'].mid.fmeasure) + ', ' + str(rouge_results_beam5_64['rouge2'].mid.fmeasure) + ', ' + str(rouge_results_beam5_64['rougeL'].mid.fmeasure) + '\n')
    stats_f.write(str(final_P) + ', ' + str(final_R) + ', ' + str(final_F1) + '\n')
    #stats_f.write(str(np.average(all_meteor_scores)) + '\n')
    stats_f.write("{} char_len, {} word_len, {} TTR, {} UTR".format(np.average(document_val_char_len),np.average(document_val_len),np.average(document_val_TTR),np.average(document_val_UTR)))
stats_f.close() 
print("Overall average results/stats written to file")

"""# Calculate Statistical Significance P-Values"""

#IGNORE THIS CELL UNLESS CALCULATING STATISTICAL SIGNIFICANCES
#modified : cell inserted to calculate p-value (paired two-tailed t-test) of individual ROUGE/BERTScore/etc. results between two sets of results

import scipy
from scipy import stats
import numpy as np
import json

# load individual metrics values from two files: first for original generations, second for perturbation generations
def calculate_pvalues_from_jsons(orig_fn,perturb_fn):
    with open(orig_fn) as orig_f:
        orig_json = json.loads(orig_f.read())
    with open(perturb_fn) as perturb_f:
        perturb_json = json.loads(perturb_f.read())
    # calculate p-values (from paired two-tailed t-tests) for each key (e.g. ROUGE-2 and ROUGE-L):
    avg_dict = {}
    sig_dict = {}
    for key in orig_json.keys():
        #print(key)
        #print(orig_json[key])
        #print(perturb_json[key],'\n')
        avg_dict[key] = (np.average(orig_json[key]),np.average(perturb_json[key])) #get average metrics per key (for sanity check - should be approximately the same as results we already have), note: only approximately due to rounding/etc.
        sig_dict[key] = scipy.stats.ttest_rel(orig_json[key],perturb_json[key])
    return avg_dict, sig_dict

orig_fn = 'commongen_data/commongen_dev_ROUGE.json' #CHANGE appropriately (file with original scores)
perturb_fn = 'trained_commongen_T5_models/steven_colab/val_generated_steven_colab_cp5797_ROUGE.json' #CHANGE appropriately (file with new scores)
#avg_dict, sig_dict = calculate_pvalues_from_jsons(orig_fn,perturb_fn) #uncomment this line to calculate the stat sig
#print(avg_dict)
#print(sig_dict)

#Example output (want pvalues to be below 0.05 for significance)
#{'length': (12.576157292185167, 9.777501244400199), 'TTR': (0.9457185788044364, 0.95752620015624), 'UTR': (0.9982761786587853, 0.9998340799573436)}
#{'length': Ttest_relResult(statistic=44.18127038233726, pvalue=0.0), 'TTR': Ttest_relResult(statistic=-9.661362129784136, pvalue=7.590963100695351e-22), 'UTR': Ttest_relResult(statistic=-7.283290524775888, pvalue=3.9006360471671226e-13)}

#IGNORE THIS CELL UNLESS CALCULATING STATISTICAL SIGNIFICANCES
# for two .txt instead of .json files (e.g. perplexity results)
def calculate_pvalues_from_txt(orig_fn,perturb_fn):
    with open(orig_fn) as orig_f:
        orig_json = [float(x.strip()) for x in orig_f.readlines()]
    with open(perturb_fn) as perturb_f:
        perturb_json = [float(x.strip()) for x in perturb_f.readlines()]
    # calculate p-values (from paired two-tailed t-tests) for each key (e.g. ROUGE-2 and ROUGE-L):
    avg_dict = {}
    sig_dict = {}
    avg_dict['PPL'] = (np.average(orig_json),np.average(perturb_json)) #get average metrics per key (for sanity check - should be approximately the same as results we already have), note: only approximately due to rounding/etc.
    sig_dict['PPL'] = scipy.stats.ttest_rel(orig_json,perturb_json)
    return avg_dict, sig_dict

orig_fn = 'stat_sig/baseline/val_generated_steven_colab_cp5797_txt.txt_PPL_individual' #CHANGE appropriately (file with original scores)
perturb_fn = 'stat_sig/chai/val_generated_chai_beam_11.txt_PPL_individual' #CHANGE appropriately (file with new scores)
#avg_dict, sig_dict = calculate_pvalues_from_txt(orig_fn,perturb_fn) #uncomment this line to calculate the stat sig
#print(avg_dict)
#print(sig_dict)